{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "import os\n",
    "import re\n",
    "\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from CHlikelihood.likelihood import Likelihood\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLines(text):\n",
    "    lst = []\n",
    "    with open(text) as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.replace('\\n', '').replace('\\t', '')\n",
    "            lst.append(line)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 文本切割\n",
    "\n",
    "    段落 --> 句子集\n",
    "    句子 --> 词汇集 （分词词包 + 停用词包）\n",
    "    词汇 --> 字典（词汇：文本序号）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. 文本切割\n",
    "\"\"\"\n",
    "def para2sent(text):\n",
    "    # 替换掉情感分析无关的文本内容\n",
    "    pattern = r'[、a-zA-Z0-9_.%（）-]+'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    # 为分句做准备    \n",
    "    text = text.replace('？', '。').replace('！', '。').replace('\\u3000', '').replace('\\n', '')\n",
    "    # 去除文末的说明\n",
    "    if '责任编辑' in text:\n",
    "        text = text.split('责任编辑')[0]\n",
    "    elif '本文来自' in text:\n",
    "        text = text.split('本文来自')[0] \n",
    "    elif '文章关键词' in text:\n",
    "        text = text.split('文章关键词')[0] \n",
    "    # 分词\n",
    "    sents = text.split('。')\n",
    "    sents = [sent for sent in sents if len(sent) != 0 and '报记者' not in sent]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2word(sentence):\n",
    "    \"\"\"\n",
    "    Segment a sentence to words\n",
    "    Delete stopwords\n",
    "    \"\"\"\n",
    "    jieba.load_userdict('财经词典.txt')\n",
    "    segResult = jieba.lcut(sentence)\n",
    "    stopwords = readLines('新停用词包.txt')\n",
    "    newSent = []\n",
    "    for word in segResult:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        else:\n",
    "            newSent.append(word)\n",
    "\n",
    "    return newSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordered_word(lst):\n",
    "    \"\"\"\n",
    "    turn cutted keywords into dict.keys, and their order as dict.values.\n",
    "    \"\"\"\n",
    "    wordDict = defaultdict(int)\n",
    "    for i in range(len(lst)):\n",
    "        if wordDict[lst[i]] != 0:\n",
    "            continue\n",
    "        wordDict[lst[i]] = i \n",
    "    return wordDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 情感定位\n",
    "    针对分词后的一个句子\n",
    "\n",
    "    词汇字典 --> sentWord, notWord, degreeWord\n",
    "    sentWord[index] = sentScore\n",
    "    notWord[index] = notScore\n",
    "    degreeWord[index] = degreeScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. 情感定位\n",
    "\"\"\"\n",
    "def classifyWords(wordDict, senDoc='财经词情感得分.txt', negDoc='否定词.txt', levelDoc='程度副词_得分.txt'):\n",
    "    # (1) 情感词: dict\n",
    "    senList = readLines(senDoc)\n",
    "    senDict = defaultdict()\n",
    "    for s in senList:\n",
    "        try:\n",
    "            senDict[s.split(' ')[0]] = s.split(' ')[1]\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    # (2) 否定词: list\n",
    "    notList = readLines(negDoc)\n",
    "    # (3) 程度副词: dict\n",
    "    degreeList = readLines(levelDoc)\n",
    "    degreeDict = defaultdict()\n",
    "    for d in degreeList:\n",
    "        try:\n",
    "            degreeDict[d.split(',')[0]] = d.split(',')[1]\n",
    "        except Exception as e:\n",
    "            print('adverb Dict error: %s' %(e))\n",
    "    \n",
    "    senWord = defaultdict()\n",
    "    notWord = defaultdict()\n",
    "    degreeWord = defaultdict()\n",
    "    \n",
    "    for word in wordDict.keys():\n",
    "        if word in senDict.keys() and word not in notList and word not in degreeDict.keys():\n",
    "            senWord[wordDict[word]] = senDict[word]  # 映射出（地址：senti得分）\n",
    "        elif word in notList and word not in degreeDict.keys():\n",
    "            notWord[wordDict[word]] = -1    # 映射出（地址：否定）\n",
    "        elif word in degreeDict.keys():\n",
    "            degreeWord[wordDict[word]] = degreeDict[word]  # 映射出（地址：程度）\n",
    "    return senWord, notWord, degreeWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 情感聚合\n",
    "    根据senWord, notWord, degreeWord计算分词后的情感得分。\n",
    "    计算方法中依赖了词语间的排序，并未使用依存分析的结论。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. 情感聚合\n",
    "\"\"\"\n",
    "def scoreSent(senWord, notWord, degreeWord):\n",
    "    score = 0\n",
    "    # 存所有情感词的位置的列表\n",
    "    senLoc = list(senWord.keys())\n",
    "    notLoc = list(notWord.keys())\n",
    "    degreeLoc = list(degreeWord.keys())\n",
    "    senloc = -1\n",
    "    \n",
    "    # 遍历句中所有情感单词senWord，sent_score_word = sent_word * degree * not\n",
    "    for i in senLoc:\n",
    "        W = 1\n",
    "        senloc += 1\n",
    "        W *= float(senWord[i])\n",
    "\n",
    "        for j in range(senLoc[senloc-1], senLoc[senloc]):\n",
    "            if j in notLoc:\n",
    "                W *= -1\n",
    "            if j in degreeLoc:\n",
    "                if W < 0:\n",
    "                    W *= float(degreeWord[j])**-1  # 程序词在否定意义上表示消弱程度，而非加强，因此取倒数\n",
    "                else:\n",
    "                    W *= float(degreeWord[j])\n",
    "                \n",
    "        # 调整权重, sent_score_sentence = sum(sent_score_word)\n",
    "        score += W\n",
    "            \n",
    "        # i定位至下一个情感词\n",
    "        i += 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(sentences) < 4:\n",
    "#     all is important\n",
    "# elif len(sentences) < 10:\n",
    "#     first & last & top(1) TFIDF\n",
    "# else:\n",
    "#     len(sentences) * 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按句分开的情感计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "综合以上的函数，按分句汇总计算感情\n",
    "\"\"\"\n",
    "\n",
    "def calc_senti(text):\n",
    "    sents = para2sent(text)  # turn text into sentences\n",
    "    lst = []\n",
    "    for sent in sents:\n",
    "        lst_word = sent2word(sent)  # turn each sentence into list of keywords\n",
    "        dct_word = ordered_word(lst_word) # turn each sentence into dict of keywords with value as index\n",
    "        senWord, notWord, degreeWord = classifyWords(dct_word) \n",
    "        lst.append(scoreSent(senWord, notWord, degreeWord))\n",
    "    return np.array(lst).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 还需要解决的问题\n",
    "0. 句子的依存分析，判断主从关系\n",
    "        （没学会；目前来看，也没有必要））\n",
    "1. 关键词重复时，wordDict无法记录  \n",
    "        (一个句子中出现重复词的可能很小，影响不大）\n",
    "2. 没有记录title中的权重  \n",
    "        (给title内容加3倍权重，具体的是senWord中的value * 3，这个系数3未来是可能优化的）\n",
    "3. 一个段落中不同的句子权重不同，因此需要添加一个给句子计算权重的函数\n",
    "4. 整理分词以及停用词的词库（针对金融） \n",
    "        （载入”财经词典.txt“）\n",
    "5. 载入金融舆情分析词库\n",
    "        （载入”财经词情感得分.txt“）\n",
    "6. 模型评估将要使用的带标记数据\n",
    "\n",
    "7. 余弦相似度的计算方法，需要根据业务需求，定制一份"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_senti_weight(senWord):\n",
    "    senTitle = dict()\n",
    "    for key in senWord.keys():\n",
    "        value = float(senWord[key]) * 3\n",
    "        senTitle[key] = value\n",
    "    return senTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_senti(title, text):\n",
    "    # text score\n",
    "    sents = para2sent(text)  # turn text into sentences\n",
    "    lst = []\n",
    "    for sent in sents:\n",
    "        lst_word = sent2word(sent)  # turn each sentence into list of keywords\n",
    "        dct_word = ordered_word(lst_word) # turn each sentence into dict of keywords with value as index\n",
    "        senWord, notWord, degreeWord = classifyWords(dct_word) \n",
    "        lst.append(scoreSent(senWord, notWord, degreeWord))\n",
    "    \n",
    "    # title score\n",
    "    lst_word = sent2word(title)\n",
    "    dct_word = ordered_word(lst_word)\n",
    "    senWord, notWord, degreeWord = classifyWords(dct_word) \n",
    "    senTitle = title_senti_weight(senWord)\n",
    "    lst.append(scoreSent(senTitle, notWord, degreeWord))\n",
    "    return np.array(lst).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(company, time):\n",
    "    conn = pymysql.connect(host='localhost', port=3306, user='root', passwd='zz6901877', db='big_train',\n",
    "                           charset=\"utf8\", use_unicode=True)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    sql = 'select title, content from sina WHERE company = \"{company}\" and type=\"news\" and time LIKE \"{time}%\"'.format(company=company, time=time)\n",
    "    cursor.execute(sql)\n",
    "    results = cursor.fetchall()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_data('aoma', '2019-04-08')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senti_doc(result):\n",
    "    # get title and content text\n",
    "    title = result[0]\n",
    "    content = main_text(result[1])\n",
    "    \n",
    "    main_content = main_text(content, topK=5)\n",
    "    senti = calc_senti(title, main_content)\n",
    "    return senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senti_day(results):\n",
    "    denominator = len(results)\n",
    "    numerator = 0\n",
    "    for result in results:\n",
    "        numerator += senti_doc(result)\n",
    "    score = int(numerator / denominator)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-19"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_day(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "得出最大tfidf列表\n",
    "\"\"\"\n",
    "def content4tfidf(content):\n",
    "    new_content = ''\n",
    "    sentences = para2sent(content)\n",
    "    for sentence in sentences:\n",
    "        words = sent2word(sentence)\n",
    "        new_content += ' '.join(words)\n",
    "    return new_content\n",
    "\n",
    "def sentence_tfidf(content, topK=5):\n",
    "    \"\"\"\n",
    "    调用content4tfidf;\n",
    "    \n",
    "    return: a string contents the main keywords in the content text\n",
    "    \"\"\"\n",
    "    content = content4tfidf(content)\n",
    "    jieba.analyse.set_idf_path('财经词典.txt')\n",
    "    result = jieba.analyse.extract_tags(new_content, topK=topK)\n",
    "    text_essence = ' '.join(result)\n",
    "    return text_essence\n",
    "\n",
    "def level_likelihood(content, topK=5):\n",
    "    \"\"\"\n",
    "    调用sentence_tfidf;\n",
    "    \n",
    "    return: a list of tuple, (likelyhood, sentences_index)\n",
    "    \"\"\"\n",
    "    # 初始化余弦相似度计算器\n",
    "    a = Likelihood()\n",
    "    # 获取所有句子\n",
    "    sentences = para2sent(content)\n",
    "    # 获取文章主干\n",
    "    text_essence = sentence_tfidf(content, topK=topK)\n",
    "    # 计算并保存\n",
    "    alike_lst = []\n",
    "    i = 0\n",
    "    for sentence in sentences:\n",
    "        alike_value = a.likelihood(sentence, text_essence)\n",
    "        alike_lst.append((alike_value, i))  # 相似度, 文章中句子index\n",
    "        i += 1\n",
    "        \n",
    "    result = sorted(alike_lst, reverse=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_text(sentences):\n",
    "    text_len = int(len(sentences) * 0.3) - 2\n",
    "    s = ''\n",
    "    for i in range(text_len):\n",
    "        sentence_index = like_result[i][1]\n",
    "        s += sentences[sentence_index]\n",
    "    return s\n",
    "\n",
    "def main_text(content, topK=5):\n",
    "    \"\"\"\n",
    "    调用 content_text\n",
    "    return: text relevant to the theme according to tfidf algorithm\n",
    "    \"\"\"\n",
    "    sentences = para2sent(content)\n",
    "    like_result = level_likelihood(content, topK=topK)\n",
    "    if len(sentences) < 4:\n",
    "        text = ''.join(sentences)\n",
    "    elif len(sentences) < 10:\n",
    "        text = sentences[0] + sentences[like_result[0][1]] + sentences[like_result[1][1]]\n",
    "    else:\n",
    "        text = content_text(sentences)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
